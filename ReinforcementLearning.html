<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="UTF-8">   
        <meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no">
        <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
        <meta name="HandheldFriendly" content="true">   
        <link href="./css/MicrosoftWord.css" rel="stylesheet">
        <link href="./css/Global.css" rel="stylesheet">
    </head>
    <body>
        <main> 
             <div id='top'>
                <div id='about'>
                    <img src="https://64.media.tumblr.com/482ba2e6e3136f986e765bb8dd8c5ffe/540d246794bf7091-d5/s540x810/6e1c994f78c1eb5006144767074ba2322b7fd318.png" alt='word icon'></img>
                    <p>About - Microsoft Word</p>
                </div>
                <div>
                    <button>_</button>
                    <button>O</button>
                    <button>X</button>
                </div> 
            </div>

             <div class='mocknav'>
                <div>
                    <div class="rectangle">i</div>
                    <p>File</p>
                    <p>Edit</p>
                    <p>View</p>
                    <p>Insert</p>
                    <p>Format</p>
                    <p>Tools</p>
                    <p>Table</p>
                    <p>Help</p>
                </div> 
                <button>x</button> 
            </div> 
             <div> 
                <div class='mocknav'>
                    <div class="rectangle">i</div>
                    <div>
                        <img id='img' src="http://i.imgur.com/6RErgJC.png"/>
                        <img id='img' src="./resources/sticker-png-windows-xp-folders-pack-256-folder-opened-256-icon-thumbnail-removebg-preview.png"/>
                        <img id='img' src="./resources/floppy-disk-removebg-preview.png"/> 
                        <img id='img' src="./resources/email-icon.drawio.png"/> 
                    </div> 
                    <div style="padding-left: 5px; border-left: 1px solid grey;">
                        <img id='img' src="./resources/printer-removebg-preview.png"/>
                        <img id='img' src="./resources/look-removebg-preview.png"/>
                        <img id='img' src="./resources/check-removebg-preview.png"/>
                    </div>
                    <div style="padding-left: 5px; border-left: 1px solid grey;">
                        <img id='img' src="./resources/scissors-removebg-preview.png"/>
                        <img id='img' src="./resources/papers-removebg-preview.png"/>
                        <img id='img' src="./resources/suitcase-removebg-preview.png"/>
                    </div>
                    <div style="padding-left: 5px; border-left: 1px solid grey;">
                        <img id='img' src="./resources/earth-removebg-preview.png"/>
                        <img id='img' src="./resources/calendar-removebg-preview.png"/> 
                    </div>
                    <div style="display: flex; justify-content: space-between; border-left: 1px solid grey; padding-left: 5px; align-items: center;">
                        <div>
                            <img id="img" style="height: 48px;" src="./resources/zoom.png"/>
                            <img id="img" src="./resources/question-removebg-preview.png"/>    
                        </div>
                        <img id="img" style="margin-left: 2rem" src="./resources/arrows-removebg-preview.png"/>
                    </div> 
                </div>
            </div> 
            <div> 
                <div class='mocknav'>
                    <div class="rectangle">i</div>
                    <div>
                        <img id="img" style="height: 8rem;" src="./resources/font-removebg-preview.png"/> 
                        <img id="img" style="height: 48px;" src="./resources/font-size-removebg-preview.png"/>
                    </div> 
                    <div>
                        <p>B</p>    
                        <p>I</p>
                        <p>U</p>
                    </div>
                    <div>
                        <img id='img' src="./resources/font-direction-left-removebg-preview.png"/>
                        <img id='img' src="./resources/font-direction-center-removebg-preview.png"/>
                    </div>
                    <div>
                        <img id='img' src="./resources/numbered-removebg-preview.png"/>
                        <img id='img' src="./resources/square-removebg-preview.png"/>
                        <img id='img' src="./resources/direction-left-removebg-preview.png"/>
                        <img id='img' src="./resources/direction-right-removebg-preview.png"/>
                    </div>

                    <div>
                        <img id='img' src="./resources/dash-square-removebg-preview.png"/>
                    </div>
                    <div class="arrows">
                        <img id="img" src="./resources/arrows-removebg-preview.png"/>
                    </div>
                </div>
            </div>

            <section> 
                <h1>Frozen Lake Environment and Cartpole</h1>
                <img src="./resources/reinforcementlearning/screenshot1.jpg"/>
                <p>What is reinforcement learning and how to use it to solve the Frozen Lake and Cartpole problems?</p>
                <h2>Frozen Lake</h2>
                <p>Frozen Lake is a simple environment composed of tiles, where the AI has to move from an initial tile to a goal. Tiles can be a safe frozen lake ‚úÖ, or a hole ‚ùå that gets you stuck forever. The AI, or agent, has 4 possible actions: go ‚óÄÔ∏èLEFT, üîΩDOWN, ‚ñ∂Ô∏èRIGHT, or üîºUP.</p>
                <h2>Cartpole</h2>
                <p>The cartpole problem is an inverted pendulum problem where a stick is balanced upright on a cart. The cart can be moved left or right and the goal is to keep the stick from falling over. A positive reward of +1 is received for every time step that the stick is upright.</p>
                <h2>Tabular Learning</h2>
                <p>Tubular learning is a subfield of AI and semi-supervised learning. It learns to act in an environment to minimize the optimal cumulative reward. It uses an agent and environment to interact with each other via episodes. The agent has no control over the environment and is in state St, meaning it can be in multiple states. The agent has a reward. It needs to choose an action that is taken as an input and it moves the agent according to the action.</p>
                <ul>
                    <li>Every time the agent takes a step a percept is created.</li>
                    <li>All the things an agent does are collected in an episode</li>
                    <li>An episode is a collection of percepts (what it did, what it will do next, rewards, etc.)</li>
                    <li>The episode ends when the agent gets to an end. The environment will be reset and the agent will start learning again.</li>
                </ul>
                <h3>Markov Decision Precess</h3>
                <p>The Markov decision process is a math model of the environment where the agent has to think of the environment mathematically to learn how to behave. It represents a collection of states, every state having a number of actions.</p>
                <p>A table is created that stores the state and the action that has a reward (R is a reward for r(s, a)). s is the state and a is the action used.</p>
                <p>A transition model is a probability distribution, meaning that there are chances that you will end up in a state, given the current state and an action (all probabilities for all combinations)</p>
                <pre>
                    <code>
                        P(s‚Äô | s, a)
                        - policy is a collection of actions (what actions to take in a certain state)
                        - non-deterministic = if you want to go up there‚Äôs an 80% chance that you will go up 
                        and 10% chance that you will go right and a 10% chance that you will go left
                    </code>
                </pre>
                <p>The random policy (œÄ)* means that the agent has an uniform distribution of chances for each direction (25%), and there will be a reward only at the end.</p>
                <pre>
                    <code>
                        œÄ(a|s) 
                    </code>
                </pre>
                <p>The optimal policy (œÄ*)* means that the agent has to learn the most optimal action by using reinforcement learning. This is implemented by walking around and keeping track of some values using algorithms.</p>
                <pre>
                    <code>
                        œÄ*(a|s) 
                        For every episode/step(s) the table is going to be updated.
                    </code>
                </pre>
                <h3>Agent</h3>
                <p>The inputs *St*, *Rt*, and the output *At* are random variables. At every timestamp, a new random variable is decided by the distribution.</p>
                <p>An agent can select an action according to a œÄ() and appears in *A*, and the rewards are determined by faith.</p>
                <p>The agent has no knowledge of the environment and needs to learn the underlying mechanics of the environment by experimenting. It refines the œÄ to œÄ* through interacting with the env.</p>
                <p>A percept is what the agent perceives and is one SARS‚Äô.</p>
                <p>A percept (Ot) contains:</p>
                <ul>
                    <li>current state</li>
                    <li>chosen action</li>
                    <li>reward</li>
                    <li>next state</li>
                    <li>done</li>  
                </ul>
                <p>An episode has a series of percepts. For each percept a return *Gt* is calculated. This *Gt* represents the discounted sum of rewards from *step t* in Episode starting from *time t*.</p>
                <pre>
                    <code>
                        return = next reward + Œ≥ + next reward
                    </code>
                </pre>
                <p>The *v-value* specifies what the total reward value should be for a particular state. The higher the better.</p>
                <h3>Agent Algorithm</h3>
                <p>The agent needs to know how to learn (what learning strategy it should choose). It also keeps track of a counter and as long as it‚Äôs not done creating episodes it will try a new Episode object. The episode stores the percept and does the cumulative returns (container for the percepts).</p>
                <p>It asks the container the location of the agent by using an input (state) and the environment tells the agent where it is. Then, it checks if the episode is done and asks the strategy what the agent should do. The agent receives the action and gives it to an input.</p>
                <p>The strategy learns from an episode or percept and it updates the state.</p>
                <h3>Tabular methods</h3>
                <p>The optimal policy can be done using tables.</p>
                <ul>
                    <li>if they need neural networks ‚Üí approximative methods</li>
                    <li>Q-Learning, N-Step Q-Learning, and Monte Carlo are temporal difference learning. They work for smaller problems with a limited number of states.</li>
                </ul>
                <p>The policy iteration is what makes the agent learn. The agent keeps track of the œÄ policy.</p>
                <p>The q-table is the v-table with more precision. The v-table updates the policy.</p>
                <ul>
                    <li>The agent starts with a random set of tables and every state has uniform distribution.</li>
                </ul> 
                <h3>Policy Evaluation</h3>
                <p>Policy evaluation is the mathematical way to drive the œÄ into a V (state value table). The agent has a policy that it keeps track of. We derive the œÄ using Bellmans‚Äôs equation. œÄ loops over the state and computes its V value from the policy table.</p>
                <pre>
                    <code>
                        V(s)=maxa(R(s,a)+ Œ≥V(s‚Äô))
                    </code>
                </pre>
                <p>Œ≥ is the discount factor. This represents how much you discount a future reward. It takes into account that future rewards are not as big as they are right now.</p>
                <ul>
                    <li>state-action values = QœÄ(s, a)</li>
                    <li>Q-values are more specific (computes a value for all the states you can be in)</li>
                    <li>V-values are associated with the entire square (how valuable it is)</li>
                    <li>all squares have their Q-table (16x4 for higher resolution)</li>
                </ul>
                <p>The v-value of a state is the average discounted sum of rewards from along the episode. It determines the quality of the path.</p>
                <p>For all the states S calculates the utility value using the value function VœÄ(s).</p>
                <h2>Approximate methods</h2>
                <p>They handle problems with more states than tabular learning. It uses function approximators (neural networks) and is suitable for continuous spaces.</p>
                <h2>Cartpole Environment</h2>
                <ul>
                    <li>has a continuous state space</li>
                    <li>two discrete states(right or left)</li>
                    <li>all observations are assigned a uniformly random value</li>
                    <li>the episode ends when:</li>
                        <ul>
                            <li>pole angle >= +12 degrees</li>
                            <li>cart position >= +-2.4 degrees (reaches the end of the display)</li>
                            <li>episode length >= 500</li>
                        </ul> 
                    <li>reward for every timestamp</li>
                    <li>solved when the pole can be held upright</li>
                </ul>
                <p>There is a reward for every timestamp, and it‚Äôs solved when the pole can be held upright without violating one of the termination conditions for at least 195 steps.</p>
                <h2>Policy Iteration</h2>
                <p>For more complex problems with many states, use a policy function that can generalize over stats when determining Q or V values. Neural networks are trained with more examples by generalizing unseen states.</p>
                <p>œÄ = neural network</p>
                <ul>
                    <li>to make the q-value equation optimal take the maximum q-value</li>
                    <li>use an expectation *E* to derive Q*(s, a)</li>
                    <li>there are infinite states possible</li>
                </ul>
                <p>The q-network is a function that you train. It minimizes the loss *L* using gradient descent. It steers the NN in the right direction. The equation can be used to create the training set for the NN.</p>
                <h2>Policy</h2>
                <p>Neural Networks are functions that generalize well-overseen examples. They are trained using seen examples and they can predict unseen ones.</p>
                <ul>
                    <li>The policy is the neural network.</li>
                    <li>The prediction step is equal to the Policy Evaluation.</li>
                </ul>
                <p>The current policy is defined by weights in a neural network, by learning the weight it also learns the policy.</p>
                <ul>
                    <li>The NN tries to minimize the error by updating the data. The NN has control only over the weights which tunes its output.</li>
                </ul>
                <h2>Learning the Q-values</h2>
                <ul>
                    <li>yt = target at time t</li>
                    <li>y^t = prediction of the NN at time t</li>
                </ul>
                <p>The target is the bait used to act. The NN uses the previous version of itself to better predict the output.</p>
                <pre>
                    <code>
                        yt = Es‚Äô[r + Œ≥ max q(s‚Äô, a‚Äô; Ot-1)]
                    </code>
                </pre>
                <ul>
                    <li>y^t prediction = output of the NN using the current weights.</li>
                </ul>
                <p>Updating the weights can be computing the gradient ‚ñºL(Ot) of the L(Ot).</p>
                <p>Use two q-networks:</p>
                <ul>
                    <li>advantage of putting quality values of s, a</li>
                    <li>the second network will help build training sets for the first one</li>
                    <li>each network has its weights</li>
                    <li>every C step will copy the weight O1</li>
                    <li>they reduce the variance in the output through a smoothing effect</li>
                </ul>
                <p>Replay memory is a collection of percepts that the agent has seen. Append percepts to replay memory in Learning Strategy.</p>
                <h2>Policy improvement</h2>
                <p>If a random number is smaller than ‚Ñá, then explore by sampling a random action from the environment, otherwise give the best prediction.</p>
                <ul>
                    <li>Don‚Äôt render every timestep (maybe every 1000 E).</li>
                </ul> 
                <h2>Features</h2>
                <p>OpenAI Gym: a Pythonic API that provides simulated training environments to train and test reinforcement learning agents.</p>
                <h2>Git code</h2>
                <p>Source to the GitHub code: <a href="https://github.com/alecsiuh/reinforcement-learning.git">https://github.com/alecsiuh/reinforcement-learning.git</a></p>
            </section>

            <div> 
                <div class='mocknav'>
                    <div class="rectangle">i</div>
                    <div>
                        <p>Draw &#9662;</p>
                        <img id="img" src="./resources/cursor-removebg-preview.png"/>
                        <img id="img"src="./resources/reload-removebg-preview.png"/>
                    </div> 
                    <div>
                        <p>AutoShapes &#9662;</p>
                        <div class="shape"></div>
                        <img id="img"src="./resources/arrow-removebg-preview.png"/>
                        <div class="square"></div>
                        <div class="circle"></div>
                        <img id="img"src="./resources/text-removebg-preview.png"/>
                        <img id="img"src="./resources/letters-removebg-preview.png"/>
                        <img id="img"src="./resources/clown-removebg-preview.png"/>
                    </div>
                    <div>
                        <img id="img"src="./resources/paint-bucket-removebg-preview.png"/>
                        <img id="img"src="./resources/brush-removebg-preview.png"/>
                        <img id="img"src="./resources/letter-removebg-preview.png"/>
                    </div>
                    <div class="arrows">
                        <img id="img" src="./resources/arrows-removebg-preview.png"/>
                    </div>
                </div>
            </div>
            <div style="background-color: #e3dac9;"> 
                <div id='bottom'>
                    <div style="width: 25rem;">
                        <p>Page 1</p>
                        <p>Sec 1</p>
                        <p>1/1</p>
                    </div> 
                    <div style="width: 50%;">
                        <p>At 1"</p>    
                        <p>Ln 1</p>
                        <p>Col 1</p>
                    </div>
                    <div style="color: grey; width: 15%;">
                        <p>REC</p>
                    </div>
                    <div style="color: grey; width: 15%;">
                        <p>TRK</p>
                    </div> 
                    <div style="color: grey; width: 15%;">
                        <p>EXT</p>
                    </div>
                </div>
            </div>
        </main>
    </body>
</html>